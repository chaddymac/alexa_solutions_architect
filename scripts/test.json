{
  "count": 65,
  "next": null,
  "previous": null,
  "results": [
    {
      "_class": "assessment",
      "id": 36264398,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company hosted a web application in an Auto Scaling group of EC2 instances. The IT manager is concerned about the over-provisioning of the resources that can cause higher operating costs. A Solutions Architect has been instructed to create a cost-effective solution without affecting the performance of the application.Which dynamic scaling policy should be used to satisfy this requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service. The size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. You can adjust its size to meet demand, either manually or by using automatic scaling.Step scaling policies and simple scaling policies are two of the dynamic scaling options available for you to use. Both require you to create CloudWatch alarms for the scaling policies. Both require you to specify the high and low thresholds for the alarms. Both require you to define whether to add or remove instances, and how many, or set the group to an exact size. The main difference between the policy types is the step adjustments that you get with step scaling policies. When step adjustments are applied, and they increase or decrease the current capacity of your Auto Scaling group, the adjustments vary based on the size of the alarm breach.",
        "relatedLectureIds": "",
        "answers": [
          "Use simple scaling.",
          "Use scheduled scaling.",
          "Use suspend and resume scaling.",
          "Use target tracking scaling."
        ]
      },
      "correct_response": ["d"],
      "section": "Design Cost-Optimized Architectures",
      "question_plain": "A company hosted a web application in an Auto Scaling group of EC2 instances. The IT manager is concerned about the over-provisioning of the resources that can cause higher operating costs. A Solutions Architect has been instructed to create a cost-effective solution without affecting the performance of the application.Which dynamic scaling policy should be used to satisfy this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264402,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An online medical system hosted in AWS stores sensitive Personally Identifiable Information (PII) of the users in an Amazon S3 bucket. Both the master keys and the unencrypted data should never be sent to AWS to comply with the strict compliance and regulatory requirements of the company. Which S3 encryption technique should the Architect use?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options:- Use an AWS KMS-managed customer master key.- Use a client-side master key.",
        "relatedLectureIds": "",
        "answers": [
          "Set up another Amazon SQS queue for the other team. Grant Amazon S3 permission to send a notification to the second SQS queue.",
          "Create a new Amazon SNS FIFO topic for the other team. Grant Amazon S3 permission to send the notification to the second SNS topic.",
          "Set up an Amazon SNS topic and configure two Amazon SQS queues to poll the SNS topic. Grant Amazon S3 permission to send notifications to Amazon SNS and update the bucket to use the new SNS topic.",
          "Create an Amazon SNS topic and configure two Amazon SQS queues to subscribe to the topic. Grant Amazon S3 permission to send notifications to Amazon SNS and update the bucket to use the new SNS topic."
        ]
      },
      "correct_response": ["d"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A company is using Amazon S3 to store frequently accessed data. When an object is created or deleted, the S3 bucket will send an event notification to the Amazon SQS queue. A solutions architect needs to create a solution that will notify the development and operations team about the created or deleted objects.Which of the following would satisfy this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264420,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A popular mobile game uses CloudFront, Lambda, and DynamoDB for its backend services. The player data is persisted on a DynamoDB table and the static assets are distributed by CloudFront. However, there are a lot of complaints that saving and retrieving player information is taking a lot of time.   To improve the game's performance, which AWS service can you use to reduce DynamoDB response times from milliseconds to microseconds?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache that can reduce Amazon DynamoDB response times from milliseconds to microseconds, even at millions of requests per second",
        "relatedLectureIds": "",
        "answers": [
          "Amazon ElastiCache",
          "AWS Device Farm",
          "DynamoDB Auto Scaling",
          "Amazon DynamoDB Accelerator (DAX)"
        ]
      },
      "correct_response": ["d"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A popular mobile game uses CloudFront, Lambda, and DynamoDB for its backend services. The player data is persisted on a DynamoDB table and the static assets are distributed by CloudFront. However, there are a lot of complaints that saving and retrieving player information is taking a lot of time.   To improve the game's performance, which AWS service can you use to reduce DynamoDB response times from milliseconds to microseconds?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264422,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "In a government agency that you are working for, you have been assigned to put confidential tax documents on AWS cloud. However, there is a concern from a security perspective on what can be put on AWS.&nbsp; What are the features in AWS that can ensure data security for your confidential documents? (Select TWO.)",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "You can secure the privacy of your data in AWS, both at rest and in-transit, through encryption. If your data is stored in EBS Volumes, you can enable EBS Encryption and if it is stored on Amazon S3, you can enable client-side and server-side encryption.Public Data Set Volume Encryption is incorrect as public data sets are designed to be publicly accessible.EBS On-Premises Data Encryption and S3 On-Premises Data Encryption are both incorrect as there is no such thing as On-Premises Data Encryption for S3 and EBS as these services are in the AWS cloud and not on your on-premises network.",
        "relatedLectureIds": "",
        "answers": [
          "EBS On-Premises Data Encryption",
          "S3 Server-Side Encryption",
          "S3 Client-Side Encryption",
          "Public Data Set Volume Encryption",
          "S3 On-Premises Data Encryption"
        ]
      },
      "correct_response": ["b", "c"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "In a government agency that you are working for, you have been assigned to put confidential tax documents on AWS cloud. However, there is a concern from a security perspective on what can be put on AWS.&nbsp; What are the features in AWS that can ensure data security for your confidential documents? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264424,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A pharmaceutical company has resources hosted on both their on-premises network and in AWS cloud. They want all of their Software Architects to access resources on both environments using their on-premises credentials, which is stored in Active Directory. In this scenario, which of the following can be used to fulfill this requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Since the company is using Microsoft Active Directory which implements Security Assertion Markup Language (SAML), you can set up a SAML-Based Federation for API Access to your AWS cloud. In this way, you can easily connect to AWS using the login credentials of your on-premises network.",
        "relatedLectureIds": "",
        "answers": [
          "Set up SAML 2.0-Based Federation by using a Web Identity Federation.",
          "Set up SAML 2.0-Based Federation by using a Microsoft Active Directory Federation Service (AD FS).",
          "Use IAM users",
          "Use Amazon VPC"
        ]
      },
      "correct_response": ["b"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A pharmaceutical company has resources hosted on both their on-premises network and in AWS cloud. They want all of their Software Architects to access resources on both environments using their on-premises credentials, which is stored in Active Directory. In this scenario, which of the following can be used to fulfill this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264426,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "A telecommunications company is planning to give AWS Console access to developers. Company policy mandates the use of identity federation and role-based access control. Currently, the roles are already assigned using groups in the corporate Active Directory. In this scenario, what combination of the following services can provide developers access to the AWS console? (Select TWO.)",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "Considering that the company is using a corporate Active Directory, it is best to use AWS Directory Service AD Connector for easier integration. In addition, since the roles are already assigned using groups in the corporate Active Directory, it would be better to also use IAM Roles. Take note that you can assign an IAM Role to the users or groups from your Active Directory once it is integrated with your VPC via the AWS Directory Service AD Connector.",
        "relatedLectureIds": "",
        "answers": [
          "AWS Directory Service AD Connector",
          "AWS Directory Service Simple AD",
          "IAM Groups",
          "IAM Roles",
          "Lambda"
        ]
      },
      "correct_response": ["a", "d"],
      "section": "Design Resilient Architectures",
      "question_plain": "A telecommunications company is planning to give AWS Console access to developers. Company policy mandates the use of identity federation and role-based access control. Currently, the roles are already assigned using groups in the corporate Active Directory. In this scenario, what combination of the following services can provide developers access to the AWS console? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264428,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An application that records weather data every minute is deployed in a fleet of Spot EC2 instances and uses a MySQL RDS database instance. Currently, there is only one RDS instance running in one Availability Zone. You plan to improve the database to ensure high availability by synchronous data replication to another RDS instance. Which of the following performs synchronous data replication in RDS?",
        "feedbacks": ["", "", "", ""],
        "explanation": "When you create or modify your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby in order to keep both Ain sync and protect your latest database updates against DB instance failure.",
        "relatedLectureIds": "",
        "answers": [
          "RDS DB instance running as a Multi-AZ deployment",
          "RDS Read Replica",
          "DynamoDB Read Replica",
          "CloudFront running as a Multi-AZ deployment"
        ]
      },
      "correct_response": ["a"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "An application that records weather data every minute is deployed in a fleet of Spot EC2 instances and uses a MySQL RDS database instance. Currently, there is only one RDS instance running in one Availability Zone. You plan to improve the database to ensure high availability by synchronous data replication to another RDS instance. Which of the following performs synchronous data replication in RDS?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264430,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A startup is using Amazon RDS to store data from a web application. Most of the time, the application has low user activity but it receives bursts of traffic within seconds whenever there is a new product announcement. The Solutions Architect needs to create a solution that will allow users around the globe to access the data using an API.What should the Solutions Architect do meet the above requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. With Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app.The first time you invoke your function, AWS Lambda creates an instance of the function and runs its handler method to process the event. When the function returns a response, it stays active and waits to process additional events. If you invoke the function again while the first event is being processed, Lambda initializes another instance, and the function processes the two events concurrently. As more events come in, Lambda routes them to available instances and creates new instances as needed. When the number of requests decreases, Lambda stops unused instances to free up the scaling capacity for other functions.",
        "relatedLectureIds": "",
        "answers": [
          "Create an API using Amazon API Gateway and use the Amazon ECS cluster with Service Auto Scaling to handle the bursts of traffic in seconds.",
          "Create an API using Amazon API Gateway and use Amazon Elastic Beanstalk with Auto Scaling to handle the bursts of traffic in seconds.",
          "Create an API using Amazon API Gateway and use AWS Lambda to handle the bursts of traffic in seconds.",
          "Create an API using Amazon API Gateway and use an Auto Scaling group of Amazon EC2 instances to handle the bursts of traffic in seconds."
        ]
      },
      "correct_response": ["c"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A startup is using Amazon RDS to store data from a web application. Most of the time, the application has low user activity but it receives bursts of traffic within seconds whenever there is a new product announcement. The Solutions Architect needs to create a solution that will allow users around the globe to access the data using an API.What should the Solutions Architect do meet the above requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264432,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An online cryptocurrency exchange platform is hosted in AWS which uses ECS Cluster and RDS in Multi-AZ Deployments configuration. The application is heavily using the RDS instance to process complex read and write database operations. To maintain the reliability, availability, and performance of your systems, you have to closely monitor how the different processes or threads on a DB instance use the CPU, including the percentage of the CPU bandwidth and total memory consumed by each process.   Which of the following is the most suitable solution to properly monitor your database?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console, or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice. By default, Enhanced Monitoring metrics are stored in the CloudWatch Logs for 30 days. To modify the amount of time the metrics are stored in the CloudWatch Logs, change the retention for the ",
        "relatedLectureIds": "",
        "answers": [
          "Use Amazon CloudWatch to monitor the CPU Utilization of your database.",
          "Create a script that collects and publishes custom metrics to CloudWatch, which tracks the real-time CPU Utilization of the RDS instance, and then set up a custom CloudWatch dashboard to view the metrics.",
          "Enable Enhanced Monitoring in RDS.",
          "Check the  metrics which are readily available in the Amazon RDS console that shows the percentage of the CPU bandwidth and total memory consumed by each database process of your RDS instance."
        ]
      },
      "correct_response": ["c"],
      "section": "Design Resilient Architectures",
      "question_plain": "An online cryptocurrency exchange platform is hosted in AWS which uses ECS Cluster and RDS in Multi-AZ Deployments configuration. The application is heavily using the RDS instance to process complex read and write database operations. To maintain the reliability, availability, and performance of your systems, you have to closely monitor how the different processes or threads on a DB instance use the CPU, including the percentage of the CPU bandwidth and total memory consumed by each process.   Which of the following is the most suitable solution to properly monitor your database?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264434,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A Solutions Architect needs to make sure that the On-Demand EC2 instance can only be accessed from this IP address (110.238.98.71) via an SSH connection. Which configuration below will satisfy this requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC can be assigned to a different set of security groups",
        "relatedLectureIds": "",
        "answers": [
          "Security Group Inbound Rule: Protocol – TCP. Port Range – 22, Source 110.238.98.71/32",
          "Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/32",
          "Security Group Inbound Rule: Protocol – TCP. Port Range – 22, Source 110.238.98.71/0",
          "Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/0"
        ]
      },
      "correct_response": ["a"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A Solutions Architect needs to make sure that the On-Demand EC2 instance can only be accessed from this IP address (110.238.98.71) via an SSH connection. Which configuration below will satisfy this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264436,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "A tech company that you are working for has undertaken a Total Cost Of Ownership (TCO) analysis evaluating the use of Amazon S3 versus acquiring more storage hardware. The result was that all 1200 employees would be granted access to use Amazon S3 for storage of their personal documents. Which of the following will you need to consider so you can set up a solution that incorporates single sign-on feature from your corporate AD or LDAP directory and also restricts access for each individual user to a designated user folder in an S3 bucket? (Select TWO.)",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "The question refers to one of the common scenarios for temporary credentials in AWS. Temporary credentials are useful in scenarios that involve identity federation, delegation, cross-account access, and IAM roles. In this example, it is called enterprise identity federation considering that you also need to set up a single sign-on (SSO) capability.The correct answers are:",
        "relatedLectureIds": "",
        "answers": [
          "Use 3rd party Single Sign-On solutions such as Atlassian Crowd, OKTA, OneLogin and many others.",
          "Set up a Federation proxy or an Identity provider, and use AWS Security Token Service to generate temporary tokens.",
          "Map each individual user to a designated user folder in S3 using Amazon WorkDocs to access their personal documents.",
          "Configure an IAM role and an IAM Policy to access the bucket.",
          "Set up a matching IAM user for each of the 1200 users in your corporate directory that needs access to a folder in the S3 bucket."
        ]
      },
      "correct_response": ["b", "d"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A tech company that you are working for has undertaken a Total Cost Of Ownership (TCO) analysis evaluating the use of Amazon S3 versus acquiring more storage hardware. The result was that all 1200 employees would be granted access to use Amazon S3 for storage of their personal documents. Which of the following will you need to consider so you can set up a solution that incorporates single sign-on feature from your corporate AD or LDAP directory and also restricts access for each individual user to a designated user folder in an S3 bucket? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264438,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An application hosted in EC2 consumes messages from an SQS queue and is integrated with SNS to send out an email to you once the process is complete. The Operations team received 5 orders but after a few hours, they saw 20 email notifications in their inbox. Which of the following could be the possible culprit for this issue?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Always remember that the messages in the SQS queue will continue to exist even after the EC2 instance has processed it, until you delete that message. You have to ensure that you delete the message after processing to prevent the message from being received and processed again once the visibility timeout expires.There are three main parts in a distributed messaging system:1. The components of your distributed system (EC2 instances)2. Your queue (distributed on Amazon SQS servers)3. Messages in the queue.You can set up a system which has several components that send messages to the queue and receive messages from the queue. The queue redundantly stores the messages across multiple Amazon SQS servers.",
        "relatedLectureIds": "",
        "answers": [
          "The web application is set for long polling so the messages are being sent twice.",
          "The web application is not deleting the messages in the SQS queue after it has processed them.",
          "The web application is set to short polling so some messages are not being picked up",
          "The web application does not have permission to consume messages in the SQS queue. "
        ]
      },
      "correct_response": ["b"],
      "section": "Design High-Performing Architectures",
      "question_plain": "An application hosted in EC2 consumes messages from an SQS queue and is integrated with SNS to send out an email to you once the process is complete. The Operations team received 5 orders but after a few hours, they saw 20 email notifications in their inbox. Which of the following could be the possible culprit for this issue?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264440,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An application is hosted in an AWS Fargate cluster that runs a batch job whenever an object is loaded on an Amazon S3 bucket. The minimum number of ECS Tasks is initially set to 1 to save on costs, and it will only increase the task count based on the new objects uploaded on the S3 bucket. Once processing is done, the bucket becomes empty and the ECS Task count should be back to 1.Which is the most suitable option to implement with the LEAST amount of effort?",
        "feedbacks": ["", "", "", ""],
        "explanation": "You can use CloudWatch Events to run Amazon ECS tasks when certain AWS events occur. You can set up a CloudWatch Events rule that runs an Amazon ECS task whenever a file is uploaded to a certain Amazon S3 bucket using the Amazon S3 PUT operation. You can also declare a reduced number of ECS tasks whenever a file is deleted on the S3 bucket using the DELETE operation.First, you must create a CloudWatch Events rule for the S3 service that will watch for object-level operations – PUT and DELETE objects. For object-level operations, it is required to create a CloudTrail trail first. On the Targets section, select the “ECS task” and input the needed values such as the cluster name, task definition and the task count. You need two rules – one for the scale-up and another for the scale-down of the ECS task count.",
        "relatedLectureIds": "",
        "answers": [
          "Set up a CloudWatch Event rule to detect S3 object PUT operations and set the target to a Lambda function that will run Amazon ECS API command to increase the number of tasks on ECS. Create another rule to detect S3 DELETE operations and run the Lambda function to reduce the number of ECS tasks.",
          "Set up a CloudWatch Event rule to detect S3 object PUT operations and set the target to the ECS cluster with the increased number of tasks. Create another rule to detect S3 DELETE operations and set the target to the ECS Cluster with 1 as the Task count.",
          "Set up an alarm in CloudWatch to monitor CloudTrail since the S3 object-level operations are recorded on CloudTrail. Create two Lambda functions for increasing/decreasing the ECS task count. Set these as respective targets for the CloudWatch Alarm depending on the S3 event.",
          "Set up an alarm in CloudWatch to monitor CloudTrail since this S3 object-level operations are recorded on CloudTrail. Set two alarm actions to update ECS task count to scale-out/scale-in depending on the S3 event."
        ]
      },
      "correct_response": ["b"],
      "section": "Design Cost-Optimized Architectures",
      "question_plain": "An application is hosted in an AWS Fargate cluster that runs a batch job whenever an object is loaded on an Amazon S3 bucket. The minimum number of ECS Tasks is initially set to 1 to save on costs, and it will only increase the task count based on the new objects uploaded on the S3 bucket. Once processing is done, the bucket becomes empty and the ECS Task count should be back to 1.Which is the most suitable option to implement with the LEAST amount of effort?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264442,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A Solutions Architect is working for a company which has multiple VPCs in various AWS regions. The Architect is assigned to set up a logging system which will track all of the changes made to their AWS resources in all regions, including the configurations made in IAM, CloudFront, AWS WAF, and Route 53. In order to pass the compliance requirements, the solution must ensure the security, integrity, and durability of the log data. It should also provide an event history of all API calls made in AWS Management Console and AWS CLI. Which of the following solutions is the best fit for this scenario?",
        "feedbacks": ["", "", "", ""],
        "explanation": "An event in CloudTrail is the record of an activity in an AWS account. This activity can be an action taken by a user, role, or service that is monitorable by CloudTrail. CloudTrail events provide a history of both API and non-API account activity made through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services. There are two types of events that can be logged in CloudTrail: management events and data events. By default, trails log management events, but not data events.",
        "relatedLectureIds": "",
        "answers": [
          "Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the  parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.",
          "Set up a new CloudWatch trail in a new S3 bucket using the AWS CLI and also pass both the  parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.",
          "Set up a new CloudWatch trail in a new S3 bucket using the CloudTrail console and also pass the  parameter then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.",
          "Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the  parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies."
        ]
      },
      "correct_response": ["a"],
      "section": "Design Resilient Architectures",
      "question_plain": "A Solutions Architect is working for a company which has multiple VPCs in various AWS regions. The Architect is assigned to set up a logging system which will track all of the changes made to their AWS resources in all regions, including the configurations made in IAM, CloudFront, AWS WAF, and Route 53. In order to pass the compliance requirements, the solution must ensure the security, integrity, and durability of the log data. It should also provide an event history of all API calls made in AWS Management Console and AWS CLI. Which of the following solutions is the best fit for this scenario?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264444,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A content management system (CMS) is hosted on a fleet of auto-scaled, On-Demand EC2 instances that use Amazon Aurora as its database. Currently, the system stores the file documents that the users upload in one of the attached EBS Volumes. Your manager noticed that the system performance is quite slow and he has instructed you to improve the architecture of the system.In this scenario, what will you do to implement a scalable, high-available POSIX-compliant shared file system?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon Elastic File System (Amazon EFS) provides simple, scalable, elastic file storage for use with AWS Cloud services and on-premises resources. When mounted on Amazon EC2 instances, an Amazon EFS file system provides a standard file system interface and file system access semantics, allowing you to seamlessly integrate Amazon EFS with your existing applications and tools. Multiple Amazon EC2 instances can access an Amazon EFS file system at the same time, allowing Amazon EFS to provide a common data source for workloads and applications running on more than one Amazon EC2 instance.This particular scenario tests your understanding of EBS, EFS, and S3. In this scenario, there is a fleet of On-Demand EC2 instances that store file documents from the users to one of the attached EBS Volumes. The system performance is quite slow because the architecture doesn't provide the EC2 instances parallel shared access to the file documents.Although an EBS Volume can be attached to multiple EC2 instances, you can only do so on instances within an availability zone. What we need is high-available storage that can span multiple availability zones. Take note as well that the type of storage needed here is \"file storage\" which means that S3 is not the best service to use because it is mainly used for \"object storage\", and S3 does not provide the notion of \"folders\" too. This is why using EFS is the correct answer.",
        "relatedLectureIds": "",
        "answers": [
          "Create an S3 bucket and use this as the storage for the CMS",
          "Use EFS",
          "Upgrading your existing EBS volumes to Provisioned IOPS SSD Volumes",
          "Use ElastiCache"
        ]
      },
      "correct_response": ["b"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A content management system (CMS) is hosted on a fleet of auto-scaled, On-Demand EC2 instances that use Amazon Aurora as its database. Currently, the system stores the file documents that the users upload in one of the attached EBS Volumes. Your manager noticed that the system performance is quite slow and he has instructed you to improve the architecture of the system.In this scenario, what will you do to implement a scalable, high-available POSIX-compliant shared file system?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264446,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company needs to design an online analytics application that uses Redshift Cluster for its data warehouse. Which of the following services allows them to monitor all API calls in Redshift instance and can also provide secured data for auditing and compliance purposes?",
        "feedbacks": ["", "", "", ""],
        "explanation": "AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. By default, CloudTrail is enabled",
        "relatedLectureIds": "",
        "answers": [
          "AWS CloudTrail",
          "Amazon CloudWatch",
          "AWS X-Ray",
          "Amazon Redshift Spectrum"
        ]
      },
      "correct_response": ["a"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A company needs to design an online analytics application that uses Redshift Cluster for its data warehouse. Which of the following services allows them to monitor all API calls in Redshift instance and can also provide secured data for auditing and compliance purposes?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264490,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A popular social network is hosted in AWS and is using a DynamoDB table as its database. There is a requirement to implement a 'follow' feature where users can subscribe to certain updates made by a particular user and be notified via email. Which of the following is the most suitable solution that you should implement to meet the requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "A DynamoDB stream is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A ",
        "relatedLectureIds": "",
        "answers": [
          "Using the Kinesis Client Library (KCL), write an application that leverages on DynamoDB Streams Kinesis Adapter that will fetch data from the DynamoDB Streams endpoint. When there are updates made by a particular user, notify the subscribers via email using SNS.",
          "Create a Lambda function that uses DynamoDB Streams Kinesis Adapter which will fetch data from the DynamoDB Streams endpoint. Set up an SNS Topic that will notify the subscribers via email when there is an update made by a particular user.",
          "Set up a DAX cluster to access the source DynamoDB table. Create a new DynamoDB trigger and a Lambda function. For every update made in the user data, the trigger will send data to the Lambda function which will then notify the subscribers via email using SNS.",
          "Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to SNS Topic that will notify the subscribers via email."
        ]
      },
      "correct_response": ["d"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A popular social network is hosted in AWS and is using a DynamoDB table as its database. There is a requirement to implement a 'follow' feature where users can subscribe to certain updates made by a particular user and be notified via email. Which of the following is the most suitable solution that you should implement to meet the requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264448,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A media company has an Amazon ECS Cluster, which uses the Fargate launch type, to host its news website. The database credentials should be supplied using environment variables, to comply with strict security compliance. As the Solutions Architect, you have to ensure that the credentials are secure and that they cannot be viewed in plaintext on the cluster itself. Which of the following is the most suitable solution in this scenario that you can implement with minimal effort?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.Secrets can be exposed to a container in the following ways:- To inject sensitive data into your containers as environment variables, use the ",
        "relatedLectureIds": "",
        "answers": [
          "In the ECS task definition file of the ECS Cluster, store the database credentials using Docker Secrets to centrally manage these sensitive data and securely transmit it to only those containers that need access to it. Secrets are encrypted during transit and at rest. A given secret is only accessible to those services which have been granted explicit access to it via IAM Role, and only while those service tasks are running.",
          "Store the database credentials in the ECS task definition file of the ECS Cluster and encrypt it with KMS. Store the task definition JSON file in a private S3 bucket and ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Create an IAM role to the ECS task definition script that allows access to the specific S3 bucket and then pass the  parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials.",
          "Use the AWS Secrets Manager to store the database credentials and then encrypt them using AWS KMS. Create a resource-based policy for your Amazon ECS task execution role () and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret which contains the sensitive data, to present to the container.",
          "Use the AWS Systems Manager Parameter Store to keep the database credentials and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role () and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container."
        ]
      },
      "correct_response": ["d"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A media company has an Amazon ECS Cluster, which uses the Fargate launch type, to host its news website. The database credentials should be supplied using environment variables, to comply with strict security compliance. As the Solutions Architect, you have to ensure that the credentials are secure and that they cannot be viewed in plaintext on the cluster itself. Which of the following is the most suitable solution in this scenario that you can implement with minimal effort?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264450,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A cryptocurrency trading platform is using an API built in AWS Lambda and API Gateway. Due to the recent news and rumors about the upcoming price surge of Bitcoin, Ethereum and other cryptocurrencies, it is expected that the trading platform would have a significant increase in site visitors and new users in the coming days ahead. \n\nIn this scenario, how can you protect the backend systems of the platform from traffic spikes?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon API Gateway provides throttling at multiple levels including global and by service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds. Amazon API Gateway tracks the number of requests per second. Any request over the limit will receive a 429 HTTP response. The client SDKs generated by Amazon API Gateway retry calls automatically when met with this response. Hence, enabling throttling limits and result caching in API Gateway is the correct answer.You can add caching to API calls by provisioning an Amazon API Gateway cache and specifying its size in gigabytes. The cache is provisioned for a specific stage of your APIs. This improves performance and reduces the traffic sent to your back end. Cache settings allow you to control the way the cache key is built and the time-to-live (TTL) of the data stored for each method. Amazon API Gateway also exposes management APIs that help you invalidate the cache for each stage.",
        "relatedLectureIds": "",
        "answers": [
          "Switch from using AWS Lambda and API Gateway to a more scalable and highly available architecture using EC2 instances, ELB, and Auto Scaling.",
          "Enable throttling limits and result caching in API Gateway.",
          "Use CloudFront in front of the API Gateway to act as a cache.",
          "Move the Lambda function in a VPC."
        ]
      },
      "correct_response": ["b"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A cryptocurrency trading platform is using an API built in AWS Lambda and API Gateway. Due to the recent news and rumors about the upcoming price surge of Bitcoin, Ethereum and other cryptocurrencies, it is expected that the trading platform would have a significant increase in site visitors and new users in the coming days ahead. \n\nIn this scenario, how can you protect the backend systems of the platform from traffic spikes?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264452,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company hosts multiple applications in their VPC. While monitoring the system, they noticed that multiple port scans are coming in from a specific IP address block that is trying to connect to several AWS resources inside their VPC. The internal security team has requested that all offending IP addresses be denied for the next 24 hours for security purposes.Which of the following is the best method to quickly and temporarily deny access from the specified IP addresses?",
        "feedbacks": ["", "", "", ""],
        "explanation": "To control the traffic coming in and out of your VPC network, you can use the network access control list (ACL)",
        "relatedLectureIds": "",
        "answers": [
          "Create a policy in IAM to deny access from the IP Address block.",
          "Modify the Network Access Control List associated with all public subnets in the VPC to deny access from the IP Address block.",
          "Add a rule in the Security Group of the EC2 instances to deny access from the IP Address block.",
          "Configure the firewall in the operating system of the EC2 instances to deny access from the IP address block."
        ]
      },
      "correct_response": ["b"],
      "section": "Design Resilient Architectures",
      "question_plain": "A company hosts multiple applications in their VPC. While monitoring the system, they noticed that multiple port scans are coming in from a specific IP address block that is trying to connect to several AWS resources inside their VPC. The internal security team has requested that all offending IP addresses be denied for the next 24 hours for security purposes.Which of the following is the best method to quickly and temporarily deny access from the specified IP addresses?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264454,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company has a cloud architecture that is composed of Linux and Windows EC2 instances that process high volumes of financial data 24 hours a day, 7 days a week. To ensure high availability of the systems, the Solutions Architect needs to create a solution that allows them to monitor the memory and disk utilization metrics of all the instances.Which of the following is the most suitable monitoring solution to implement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon CloudWatch has available Amazon EC2 Metrics for you to use for monitoring CPU utilization, Network utilization, Disk performance, and Disk Reads/Writes. In case you need to monitor the below items, you need to prepare a custom metric using a Perl or other shell script, as there are no ready to use metrics for:Memory utilizationDisk swap utilizationDisk space utilizationPage file utilizationLog collection",
        "relatedLectureIds": "",
        "answers": [
          "Use the default CloudWatch configuration to EC2 instances where the memory and disk utilization metrics are already available. Install the AWS Systems Manager (SSM) Agent to all the EC2 instances.",
          "Install the CloudWatch agent to all the EC2 instances that gathers the memory and disk utilization data. View the custom metrics in the Amazon CloudWatch console.",
          "Enable the Enhanced Monitoring option in EC2 and install CloudWatch agent to all the EC2 instances to be able to view the memory and disk utilization in the CloudWatch dashboard.",
          "Use Amazon Inspector and install the Inspector agent to all EC2 instances."
        ]
      },
      "correct_response": ["b"],
      "section": "Design Resilient Architectures",
      "question_plain": "A company has a cloud architecture that is composed of Linux and Windows EC2 instances that process high volumes of financial data 24 hours a day, 7 days a week. To ensure high availability of the systems, the Solutions Architect needs to create a solution that allows them to monitor the memory and disk utilization metrics of all the instances.Which of the following is the most suitable monitoring solution to implement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264456,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "A newly hired Solutions Architect is assigned to manage a set of CloudFormation templates that are used in the company's cloud architecture in AWS. The Architect accessed the templates and tried to analyze the configured IAM policy for an S3 bucket.What does the above IAM policy allow? (Select THREE.)",
        "feedbacks": ["", "", "", "", "", ""],
        "explanation": "You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, AWS Organizations SCPs, ACLs, and session policies.IAM policies define permissions for action regardless of the method that you use to perform the operation. For example, if a policy allows the ",
        "relatedLectureIds": "",
        "answers": [
          "An IAM user with this IAM policy is allowed to read objects from all S3 buckets owned by the account.",
          "An IAM user with this IAM policy is allowed to write objects into the  S3 bucket.",
          "An IAM user with this IAM policy is allowed to change access rights for the  S3 bucket.",
          "An IAM user with this IAM policy is allowed to read objects in the  S3 bucket but not allowed to list the objects in the bucket.",
          "An IAM user with this IAM policy is allowed to read objects from the  S3 bucket.",
          "An IAM user with this IAM policy is allowed to read and delete objects from the  S3 bucket."
        ]
      },
      "correct_response": ["a", "b", "e"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A newly hired Solutions Architect is assigned to manage a set of CloudFormation templates that are used in the company's cloud architecture in AWS. The Architect accessed the templates and tried to analyze the configured IAM policy for an S3 bucket.{ \n \"Version\": \"2012-10-17\", \n \"Statement\": [ \n  { \n   \"Effect\": \"Allow\", \n   \"Action\": [ \n    \"s3:Get*\", \n    \"s3:List*\" \n   ], \n   \"Resource\": \"*\" \n  }, \n  { \n   \"Effect\": \"Allow\", \n   \"Action\": \"s3:PutObject\", \n   \"Resource\": \"arn:aws:s3:::boracay/*\" \n  } \n ] \n}What does the above IAM policy allow? (Select THREE.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264458,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A web application is using CloudFront to distribute their images, videos, and other static contents stored in their S3 bucket to its users around the world. The company has recently introduced a new member-only access to some of its high quality media files. There is a requirement to provide access to multiple private media files only to their paying subscribers without having to change their current URLs.   Which of the following is the most suitable solution that you should implement to satisfy this requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "CloudFront signed URLs and signed cookies provide the same basic functionality: they allow you to control who can access your content. If you want to serve private content through CloudFront and you're trying to decide whether to use signed URLs or signed cookies, consider the following:Use signed URLs for the following cases:- You want to use an RTMP distribution. Signed cookies aren't supported for RTMP distributions.- You want to restrict access to individual files, for example, an installation downloa/p>",
        "relatedLectureIds": "",
        "answers": [
          "Configure your CloudFront distribution to use Match Viewer as its Origin Protocol Policy which will automatically match the user request. This will allow access to the private content if the request is a paying member and deny it if it is not a member.",
          "Create a Signed URL with a custom policy which only allows the members to see the private files.",
          "Configure your CloudFront distribution to use Field-Level Encryption to protect your private data and only allow access to members.",
          "Use Signed Cookies to control who can access the private files in your CloudFront distribution by modifying your application to determine whether a user should have access to your content. For members, send the required  headers to the viewer which will unlock the content only to them."
        ]
      },
      "correct_response": ["d"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A web application is using CloudFront to distribute their images, videos, and other static contents stored in their S3 bucket to its users around the world. The company has recently introduced a new member-only access to some of its high quality media files. There is a requirement to provide access to multiple private media files only to their paying subscribers without having to change their current URLs.   Which of the following is the most suitable solution that you should implement to satisfy this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264460,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A suite of web applications is hosted in an Auto Scaling group of EC2 instances across three Availability Zones and is configured with default settings. There is an Application Load Balancer that forwards the request to the respective target group on the URL path. The scale-in policy has been triggered due to the low number of incoming traffic to the application. Which EC2 instance will be the first one to be terminated by your Auto Scaling group?",
        "feedbacks": ["", "", "", ""],
        "explanation": "The default termination policy is designed to help ensure that your network architecture spans Availability Zones evenly. With the default termination policy, the behavior of the Auto Scaling group is as follows:1. If there are instances in multiple Availability Zones, choose the Availability Zone with the most instances and at least one instance that is not protected from scale in. If there is more than one Availability Zone with this number of instances, choose the Availability Zone with the instances that use the oldest launch configuration.2. Determine which unprotected instances in the selected Availability Zone use the oldest launch configuration. If there is one such instance, terminate it.3. If there are multiple instances to terminate based on the above criteria, determine which unprotected instance",
        "relatedLectureIds": "",
        "answers": [
          "The EC2 instance which has the least number of user sessions",
          "The EC2 instance which has been running for the longest time",
          "The EC2 instance launched from the oldest launch configuration",
          "The instance will be randomly selected by the Auto Scaling group"
        ]
      },
      "correct_response": ["c"],
      "section": "Design Resilient Architectures",
      "question_plain": "A suite of web applications is hosted in an Auto Scaling group of EC2 instances across three Availability Zones and is configured with default settings. There is an Application Load Balancer that forwards the request to the respective target group on the URL path. The scale-in policy has been triggered due to the low number of incoming traffic to the application. Which EC2 instance will be the first one to be terminated by your Auto Scaling group?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264462,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company plans to build a data analytics application in AWS which will be deployed in an Auto Scaling group of On-Demand EC2 instances and a MongoDB database. It is expected that the database will have high-throughput workloads performing small, random I/O operations. As the Solutions Architect, you are required to properly set up and launch the required resources in AWS.Which of the following is the most suitable EBS type to use for your database?",
        "feedbacks": ["", "", "", ""],
        "explanation": "On a given volume configuration, certain I/O characteristics drive the performance behavior for your EBS volumes. SSD-backed volumes, such as General Purpose SSD",
        "relatedLectureIds": "",
        "answers": [
          "General Purpose SSD (gp2)",
          "Provisioned IOPS SSD (io1)",
          "Throughput Optimized HDD (st1)",
          "Cold HDD (sc1)"
        ]
      },
      "correct_response": ["b"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A company plans to build a data analytics application in AWS which will be deployed in an Auto Scaling group of On-Demand EC2 instances and a MongoDB database. It is expected that the database will have high-throughput workloads performing small, random I/O operations. As the Solutions Architect, you are required to properly set up and launch the required resources in AWS.Which of the following is the most suitable EBS type to use for your database?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264464,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A travel photo sharing website is using Amazon S3 to serve high-quality photos to visitors of your website. After a few days, you found out that there are other travel websites linking and using your photos. This resulted in financial losses for your business. What is the MOST effective method to mitigate this issue?",
        "feedbacks": ["", "", "", ""],
        "explanation": "In Amazon S3, all objects are private by default. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration.Anyone who receives the pre-signed URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a pre-signed URL.",
        "relatedLectureIds": "",
        "answers": [
          "Configure your S3 bucket to remove public read access and use pre-signed URLs with expiry dates.",
          "Use CloudFront distributions for your photos.",
          "Block the IP addresses of the offending websites using NACL.",
          "Store and privately serve the high-quality photos on Amazon WorkDocs instead."
        ]
      },
      "correct_response": ["a"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A travel photo sharing website is using Amazon S3 to serve high-quality photos to visitors of your website. After a few days, you found out that there are other travel websites linking and using your photos. This resulted in financial losses for your business. What is the MOST effective method to mitigate this issue?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264488,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A Forex trading platform, which frequently processes and stores global financial data every minute, is hosted in your on-premises data center and uses an Oracle database. Due to a recent cooling problem in their data center, the company urgently needs to migrate their infrastructure to AWS to improve the performance of their applications. As the Solutions Architect, you are responsible in ensuring that the database is properly migrated and should remain available in case of database server failure in the future.  Which of the following is the most suitable solution to meet the requirement? ",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.",
        "relatedLectureIds": "",
        "answers": [
          "Launch an Oracle database instance in RDS with Recovery Manager (RMAN) enabled.",
          "Launch an Oracle Real Application Clusters (RAC) in RDS.",
          "Create an Oracle database in RDS with Multi-AZ deployments.",
          "Convert the database schema using the AWS Schema Conversion Tool and AWS Database Migration Service. Migrate the Oracle database to a non-cluster Amazon Aurora with a single instance."
        ]
      },
      "correct_response": ["c"],
      "section": "Design Resilient Architectures",
      "question_plain": "A Forex trading platform, which frequently processes and stores global financial data every minute, is hosted in your on-premises data center and uses an Oracle database. Due to a recent cooling problem in their data center, the company urgently needs to migrate their infrastructure to AWS to improve the performance of their applications. As the Solutions Architect, you are responsible in ensuring that the database is properly migrated and should remain available in case of database server failure in the future.  Which of the following is the most suitable solution to meet the requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264466,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A multi-tiered application hosted in your on-premises data center is scheduled to be migrated to AWS. The application has a message broker service which uses industry standard messaging APIs and protocols that must be migrated as well, without rewriting the messaging code in your application.  Which of the following is the most suitable service that you should use to move your messaging service to AWS?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon MQ, Amazon SQS, and Amazon SNS are messaging services that are suitable for anyone from startups to enterprises. If you're using messaging with existing applications and want to move your messaging service to the cloud quickly and easily, it is recommended that you consider Amazon MQ. It supports industry-standard APIs and protocols so you can switch from any standards-based message broker to Amazon MQ without rewriting the messaging code in your applications.Hence, Amazon MQ is the correct answer.",
        "relatedLectureIds": "",
        "answers": ["Amazon MQ", "Amazon SQS", "Amazon SNS", "Amazon SWF"]
      },
      "correct_response": ["a"],
      "section": "Design Resilient Architectures",
      "question_plain": "A multi-tiered application hosted in your on-premises data center is scheduled to be migrated to AWS. The application has a message broker service which uses industry standard messaging APIs and protocols that must be migrated as well, without rewriting the messaging code in your application.  Which of the following is the most suitable service that you should use to move your messaging service to AWS?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264468,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company is using a combination of API Gateway and Lambda for the web services of the online web portal that is being accessed by hundreds of thousands of clients each day. They will be announcing a new revolutionary product and it is expected that the web portal will receive a massive number of visitors all around the globe.How can you protect the backend systems and applications from traffic spikes?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon API Gateway provides throttling at multiple levels including global and by a service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds.",
        "relatedLectureIds": "",
        "answers": [
          "Use throttling limits in API Gateway",
          "API Gateway will automatically scale and handle massive traffic spikes so you do not have to do anything.",
          "Manually upgrade the EC2 instances being used by API Gateway",
          "Deploy Multi-AZ in API Gateway with Read Replica"
        ]
      },
      "correct_response": ["a"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A company is using a combination of API Gateway and Lambda for the web services of the online web portal that is being accessed by hundreds of thousands of clients each day. They will be announcing a new revolutionary product and it is expected that the web portal will receive a massive number of visitors all around the globe.How can you protect the backend systems and applications from traffic spikes?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264470,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An online shopping platform is hosted on an Auto Scaling group of Spot EC2 instances and uses Amazon Aurora PostgreSQL as its database. There is a requirement to optimize your database workloads in your cluster where you have to direct the write operations of the production traffic to your high-capacity instances and point the reporting queries sent by your internal staff to the low-capacity instances. Which is the most suitable configuration for your application as well as your Aurora database cluster to achieve this requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an ",
        "relatedLectureIds": "",
        "answers": [
          "Configure your application to use the reader endpoint for both production traffic and reporting queries, which will enable your Aurora database to automatically perform load-balancing among all the Aurora Replicas.",
          "In your application, use the instance endpoint of your Aurora database to handle the incoming production traffic and use the cluster endpoint to handle reporting queries.",
          "Create a custom endpoint in Aurora based on the specified criteria for the production traffic and another custom endpoint to handle the reporting queries.",
          "Do nothing since by default, Aurora will automatically direct the production traffic to your high-capacity instances and the reporting queries to your low-capacity instances."
        ]
      },
      "correct_response": ["c"],
      "section": "Design Resilient Architectures",
      "question_plain": "An online shopping platform is hosted on an Auto Scaling group of Spot EC2 instances and uses Amazon Aurora PostgreSQL as its database. There is a requirement to optimize your database workloads in your cluster where you have to direct the write operations of the production traffic to your high-capacity instances and point the reporting queries sent by your internal staff to the low-capacity instances. Which is the most suitable configuration for your application as well as your Aurora database cluster to achieve this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264472,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company has 3 DevOps engineers that are handling its software development and infrastructure management processes. One of the engineers accidentally deleted a file hosted in Amazon S3 which has caused disruption of service.What can the DevOps engineers do to prevent this from happening again?",
        "feedbacks": ["", "", "", ""],
        "explanation": "To avoid accidental deletion in Amazon S3 bucket, you can:- Enable Versioning- Enable MFA (Multi-Factor Authentication) DeleteVersioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.If the MFA (Multi-Factor Authentication) Delete is enabled, it requires additional authentication for either of the following operations:- Change the versioning state of your bucket- Permanently delete an object versionUsing S3 Infrequently Accessed storage to store the data is incorrect. Switching your storage class to S3 Infrequent Access won't help mitigate accidental deletions.Setting up a signed URL for all users is incorrect. Signed URLs give you more control over access to your content, so this feature deals more on accessing rather than deletion.Creating an IAM bucket policy that disables delete operation is incorrect. If you create a bucket policy preventing deletion, other users won't be able to delete objects that should be deleted. You only want to prevent accidental deletion, not disable the action itself.Reference:",
        "relatedLectureIds": "",
        "answers": [
          "Use S3 Infrequently Accessed storage to store the data.",
          "Enable S3 Versioning and Multi-Factor Authentication Delete on the bucket.",
          "Set up a signed URL for all users.",
          "Create an IAM bucket policy that disables delete operation."
        ]
      },
      "correct_response": ["b"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A company has 3 DevOps engineers that are handling its software development and infrastructure management processes. One of the engineers accidentally deleted a file hosted in Amazon S3 which has caused disruption of service.What can the DevOps engineers do to prevent this from happening again?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264474,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "There are a lot of outages in the Availability Zone of your RDS database instance to the point that you have lost access to the database. What could you do to prevent losing access to your database in case that this event happens again?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. For this scenario, enabling Multi-AZ failover is the correct answer. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.",
        "relatedLectureIds": "",
        "answers": [
          "Make a snapshot of the database",
          "Enabled Multi-AZ failover ",
          "Increase the database instance size",
          "Create a read replica"
        ]
      },
      "correct_response": ["b"],
      "section": "Design Resilient Architectures",
      "question_plain": "There are a lot of outages in the Availability Zone of your RDS database instance to the point that you have lost access to the database. What could you do to prevent losing access to your database in case that this event happens again?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264476,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "An IT consultant is working for a large financial company. The role of the consultant is to help the development team build a highly available web application using stateless web servers.In this scenario, which AWS services are suitable for storing session state data? (Select TWO.)",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "DynamoDB and ElastiCache are the correct answers. You can store session state data on both DynamoDB and ElastiCache. These AWS services provide high-performance storage of key-value pairs which can be used to build a highly available web application.",
        "relatedLectureIds": "",
        "answers": [
          "Redshift Spectrum",
          "DynamoDB",
          "RDS",
          "ElastiCache",
          "Glacier"
        ]
      },
      "correct_response": ["b", "d"],
      "section": "Design Resilient Architectures",
      "question_plain": "An IT consultant is working for a large financial company. The role of the consultant is to help the development team build a highly available web application using stateless web servers.In this scenario, which AWS services are suitable for storing session state data? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264478,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "A popular social media website uses a CloudFront web distribution to serve their static contents to their millions of users around the globe. They are receiving a number of complaints recently that their users take a lot of time to log into their website. There are also occasions when their users are getting HTTP 504 errors. You are instructed by your manager to significantly reduce the user's login time to further optimize the system. Which of the following options should you use together to set up a cost-effective solution that can improve your application's performance? (Select TWO.)",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:- After CloudFront receives a request from a viewer (viewer request)- Before CloudFront forwards the request to the origin (origin request)- After CloudFront receives the response from the origin (origin response)- Before CloudFront forwards the response to the viewer (viewer response)",
        "relatedLectureIds": "",
        "answers": [
          "Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.",
          "Use multiple and geographically disperse VPCs to various AWS regions then create a transit VPC to connect all of your resources. In order to handle the requests faster, set up Lambda functions in each region using the AWS Serverless Application Model (SAM) service.",
          "Configure your origin to add a  to increase the cache hit ratio of your CloudFront distribution.",
          "Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user.",
          "Set up an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses."
        ]
      },
      "correct_response": ["a", "e"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A popular social media website uses a CloudFront web distribution to serve their static contents to their millions of users around the globe. They are receiving a number of complaints recently that their users take a lot of time to log into their website. There are also occasions when their users are getting HTTP 504 errors. You are instructed by your manager to significantly reduce the user's login time to further optimize the system. Which of the following options should you use together to set up a cost-effective solution that can improve your application's performance? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264480,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company has a hybrid cloud architecture that connects their on-premises data center and cloud infrastructure in AWS. They require a durable storage backup for their corporate documents stored on-premises and a local cache that provides low latency access to their recently accessed data to reduce data egress charges. The documents must be stored to and retrieved from AWS via the Server Message Block (SMB) protocol. These files must immediately be accessible within minutes for six months and archived for another decade to meet the data compliance. Which of the following is the best and most cost-effective approach to implement in this scenario?",
        "feedbacks": ["", "", "", ""],
        "explanation": "A file gateway supports a file interface into Amazon Simple Storage Service (Amazon S3) and combines a service and a virtual software appliance. By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). The software appliance, or gateway, is deployed into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or Linux Kernel-based Virtual Machine (KVM) hypervisor.",
        "relatedLectureIds": "",
        "answers": [
          "Launch a new file gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the file gateway and set up a lifecycle policy to move the data into Glacier for data archival.",
          "Launch a new tape gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the tape gateway and set up a lifecycle policy to move the data into Glacier for archival.",
          "Establish a Direct Connect connection to integrate your on-premises network to your VPC. Upload the documents on Amazon EBS Volumes and use a lifecycle policy to automatically move the EBS snapshots to an S3 bucket, and then later to Glacier for archival.",
          "Use AWS Snowmobile to migrate all of the files from the on-premises network. Upload the documents to an S3 bucket and set up a lifecycle policy to move the data into Glacier for archival."
        ]
      },
      "correct_response": ["a"],
      "section": "Design Resilient Architectures",
      "question_plain": "A company has a hybrid cloud architecture that connects their on-premises data center and cloud infrastructure in AWS. They require a durable storage backup for their corporate documents stored on-premises and a local cache that provides low latency access to their recently accessed data to reduce data egress charges. The documents must be stored to and retrieved from AWS via the Server Message Block (SMB) protocol. These files must immediately be accessible within minutes for six months and archived for another decade to meet the data compliance. Which of the following is the best and most cost-effective approach to implement in this scenario?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264482,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "A company conducted a surprise IT audit on all of the AWS resources being used in the production environment. During the audit activities, it was noted that you are using a combination of Standard and Convertible Reserved EC2 instances in your applications. They argued that you should have used Spot EC2 instances instead as it is cheaper than the Reserved Instance.Which of the following are the characteristics and benefits of using these two types of Reserved EC2 instances, which you can use as justification? (Select TWO.)",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "Reserved Instances (RIs) provide you with a significant discount (up to 75%) compared to On-Demand instance pricing. You have the flexibility to change families, OS types, and tenancies while benefiting from RI pricing when you use Convertible RIs. One important thing to remember here is that Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account.The offering class of a Reserved Instance is either Standard or Convertible. A Standard Reserved Instance provides a more significant discount than a Convertible Reserved Instance, but you can't exchange a Standard Reserved Instance unlike Convertible Reserved Instances. You can modify Standard and Convertible Reserved Instances. Take note that in Convertible Reserved Instances, you are allowed to exchange another Convertible Reserved instance with a different instance type and tenancy.The configuration of a Reserved Instance comprises a single instance type, platform, scope, and tenancy over a term. If your computing needs change, you might be able to modify or exchange your Reserved Instance.When your computing needs change, you can modify your Standard or Convertible Reserved Instances and continue to take advantage of the billing benefit. You can modify the Availability Zone, scope, network platform, or instance size (within the same instance type) of your Reserved Instance. You can also sell your unused instance on the Reserved Instance Marketplace.Hence, the correct options are:- Reserved Instances don't get interrupted unlike Spot instances in the event that there are not enough unused EC2 instances to meet the demand- Convertible Reserved Instances allows you to exchange for another Convertible Reserved instance with a different instance type and tenancy.The option that says: Standard Reserved Instances can be later exchanged for other Convertible Reserved Instances is incorrect because only Convertible Reserved Instances can be exchanged for other Convertible Reserved Instances.The option that says: It can enable you to reserve capacity for your Amazon EC2 instances in multiple Availability Zones and multiple AWS Regions for any duration is incorrect because you can reserve capacity to a specific AWS Region (regional Reserved Instance) or specific Availability Zone (zonal Reserved Instance) only. You cannot reserve capacity to multiple AWS Regions in a single RI purchase.The option that says: It runs in a VPC on hardware that's dedicated to a single customer is incorrect because that is the description of a Dedicated instance and not a Reserved Instance. A Dedicated instance runs in a VPC on hardware that's dedicated to a single customer.References:",
        "relatedLectureIds": "",
        "answers": [
          "Standard Reserved Instances can be later exchanged for other Convertible Reserved Instances",
          "It can enable you to reserve capacity for your Amazon EC2 instances in multiple Availability Zones and multiple AWS Regions for any duration.",
          "Reserved Instances don't get interrupted unlike Spot instances in the event that there are not enough unused EC2 instances to meet the demand.",
          "It runs in a VPC on hardware that's dedicated to a single customer.",
          "Convertible Reserved Instances allow you to exchange for another Convertible Reserved instance with a different instance type and tenancy."
        ]
      },
      "correct_response": ["c", "e"],
      "section": "Design Resilient Architectures",
      "question_plain": "A company conducted a surprise IT audit on all of the AWS resources being used in the production environment. During the audit activities, it was noted that you are using a combination of Standard and Convertible Reserved EC2 instances in your applications. They argued that you should have used Spot EC2 instances instead as it is cheaper than the Reserved Instance.Which of the following are the characteristics and benefits of using these two types of Reserved EC2 instances, which you can use as justification? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264484,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using DynamoDB. You are instructed to improve the database performance by distributing the workload evenly and using the provisioned throughput efficiently.    Which of the following would you consider to implement for your DynamoDB table?",
        "relatedLectureIds": "",
        "explanation": "The partition key portion of a table's primary key determines the logical partitions in which a table's data is stored. This in turn affects the underlying physical partitions. Provisioned I/O capacity for the table is divided evenly among these physical partitions. Therefore a partition key design that doesn't distribute I/O requests evenly can create \"hot\" partitions that result in throttling and use your provisioned I/O capacity inefficiently.The optimal usage of a table's provisioned throughput depends not only on the workload patterns of individual items, but also on the partition-key design. This doesn't mean that you must access all partition key values to achieve an efficient throughput level, or even that the percentage of accessed partition key values must be high. It does mean that the more distinct partition key values that your workload accesses, the more those requests will be spread across the partitioned space. In general, you will use your provisioned throughput more efficiently as the ratio of partition key values accessed to the total number of partition key values increases.One example for this is the use of partition keys with high-cardinality attributes, which have a large number of distinct values for each item.Reducing the number of partition keys in the DynamoDB table is incorrect. Instead of doing this, you should actually add more to improve its performance to distribute the I/O requests evenly and not avoid \"hot\" partitions.Using partition keys with low-cardinality attributes, which have a few number of distinct values for each item is incorrect because this is the exact opposite of the correct answer. Remember that the more distinct partition key values your workload accesses, the more those requests will be spread across the partitioned space. Conversely, the less distinct partition key values, the less evenly spread it would be across the partitioned space, which effectively slows the performance.The option that says: Avoid using a composite primary key, which is composed of a partition key and a sort key is incorrect because as mentioned, a composite primary key will provide more partition for the table and in turn, improves the performance. Hence, it should be used and not avoided.References:",
        "feedbacks": ["", "", "", ""],
        "answers": [
          "Reduce the number of partition keys in the DynamoDB table.",
          "Use partition keys with high-cardinality attributes, which have a large number of distinct values for each item.",
          "Use partition keys with low-cardinality attributes, which have a few number of distinct values for each item.",
          "Avoid using a composite primary key, which is composed of a partition key and a sort key."
        ]
      },
      "correct_response": ["b"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using DynamoDB. You are instructed to improve the database performance by distributing the workload evenly and using the provisioned throughput efficiently.    Which of the following would you consider to implement for your DynamoDB table?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264486,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company is designing a banking portal that uses Amazon ElastiCache for Redis as its distributed session management component. Since the other Cloud Engineers in your department have access to your ElastiCache cluster, you have to secure the session data in the portal by requiring them to enter a password before they are granted permission to execute Redis commands.As the Solutions Architect, which of the following should you do to meet the above requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Using Redis ",
        "relatedLectureIds": "",
        "answers": [
          "Set up an IAM Policy and MFA which requires the Cloud Engineers to enter their IAM credentials and token before they can access the ElastiCache cluster.",
          "Set up a Redis replication group and enable the",
          "Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the  parameters enabled.",
          "Enable the in-transit encryption for Redis replication groups."
        ]
      },
      "correct_response": ["c"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A company is designing a banking portal that uses Amazon ElastiCache for Redis as its distributed session management component. Since the other Cloud Engineers in your department have access to your ElastiCache cluster, you have to secure the session data in the portal by requiring them to enter a password before they are granted permission to execute Redis commands.As the Solutions Architect, which of the following should you do to meet the above requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264492,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A Solutions Architect identified a series of DDoS attacks while monitoring the VPC. The Architect needs to fortify the current cloud infrastructure to protect the data of the clients.Which of the following is the most suitable solution to mitigate these kinds of attacks?",
        "feedbacks": ["", "", "", ""],
        "explanation": "For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing(ELB), Amazon CloudFront, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall.",
        "relatedLectureIds": "",
        "answers": [
          "Use AWS Shield Advanced to detect and mitigate DDoS attacks.",
          "Using the AWS Firewall Manager, set up a security layer that will prevent SYN floods, UDP reflection attacks, and other DDoS attacks.",
          "Set up a web application firewall using AWS WAF to filter, monitor, and block HTTP traffic.",
          "A combination of Security Groups and Network Access Control Lists to only allow authorized traffic to access your VPC."
        ]
      },
      "correct_response": ["a"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A Solutions Architect identified a series of DDoS attacks while monitoring the VPC. The Architect needs to fortify the current cloud infrastructure to protect the data of the clients.Which of the following is the most suitable solution to mitigate these kinds of attacks?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264494,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An AI-powered Forex trading application consumes thousands of data sets to train its machine learning model. The application’s workload requires a high-performance, parallel hot storage to process the training datasets concurrently. It also needs cost-effective cold storage to archive those datasets that yield low profit.Which of the following Amazon storage services should the developer use?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Hot storage refers to the storage that keeps frequently accessed data ( hot data ). Warm storage refers to the storage that keeps less frequently accessed data ( warm data ). Cold storage refers to the storage that keeps rarely accessed data ( cold data ). In terms of pricing, the colder the data, the cheaper it is to store, and the costlier it is to access when needed.",
        "relatedLectureIds": "",
        "answers": [
          "Use Amazon FSx For Lustre and Amazon EBS Provisioned IOPS SSD (io1) volumes for hot and cold storage respectively.",
          "Use Amazon FSx For Lustre and Amazon S3 for hot and cold storage respectively.",
          "Use Amazon Elastic File System and Amazon S3 for hot and cold storage respectively.",
          "Use Amazon FSx For Windows File Server and Amazon S3 for hot and cold storage respectively."
        ]
      },
      "correct_response": ["b"],
      "section": "Design High-Performing Architectures",
      "question_plain": "An AI-powered Forex trading application consumes thousands of data sets to train its machine learning model. The application’s workload requires a high-performance, parallel hot storage to process the training datasets concurrently. It also needs cost-effective cold storage to archive those datasets that yield low profit.Which of the following Amazon storage services should the developer use?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264496,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An application consists of multiple EC2 instances in private subnets in different availability zones. The application uses a single NAT Gateway for downloading software patches from the Internet to the instances. There is a requirement to protect the application from a single point of failure when the NAT Gateway encounters a failure or if its availability zone goes down.How should the Solutions Architect redesign the architecture to be more highly available and cost-effective",
        "feedbacks": ["", "", "", ""],
        "explanation": "A NAT Gateway is a highly available, managed Network Address Translation (NAT) service for your resources in a private subnet to access the Internet. NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone.You must create a NAT gateway on a public subnet to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances.",
        "relatedLectureIds": "",
        "answers": [
          "Create a NAT Gateway in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone",
          "Create a NAT Gateway in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.",
          "Create two NAT Gateways in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.",
          "Create three NAT Gateways in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone."
        ]
      },
      "correct_response": ["a"],
      "section": "Design Resilient Architectures",
      "question_plain": "An application consists of multiple EC2 instances in private subnets in different availability zones. The application uses a single NAT Gateway for downloading software patches from the Internet to the instances. There is a requirement to protect the application from a single point of failure when the NAT Gateway encounters a failure or if its availability zone goes down.How should the Solutions Architect redesign the architecture to be more highly available and cost-effective",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264498,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company plans to launch an Amazon EC2 instance in a private subnet for its internal corporate web portal. For security purposes, the EC2 instance must send data to Amazon DynamoDB and Amazon S3 via private endpoints that don't pass through the public Internet.Which of the following can meet the above requirements?",
        "feedbacks": ["", "", "", ""],
        "explanation": "A VPC endpoint allows you to privately connect your VPC to supported AWS and VPC endpoint services powered by AWS PrivateLink without needing an Internet gateway, NAT computer, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.",
        "relatedLectureIds": "",
        "answers": [
          "Use VPC endpoints to route all access to S3 and DynamoDB via private endpoints.",
          "Use AWS VPN CloudHub to route all access to S3 and DynamoDB via private endpoints.",
          "Use AWS Transit Gateway to route all access to S3 and DynamoDB via private endpoints.",
          "Use AWS Direct Connect to route all access to S3 and DynamoDB via private endpoints."
        ]
      },
      "correct_response": ["a"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A company plans to launch an Amazon EC2 instance in a private subnet for its internal corporate web portal. For security purposes, the EC2 instance must send data to Amazon DynamoDB and Amazon S3 via private endpoints that don't pass through the public Internet.Which of the following can meet the above requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264500,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An organization needs a persistent block storage volume that will be used for mission-critical workloads. The backup data will be stored in an object storage service and after 30 days, the data will be stored in a data archiving storage service.What should you do to meet the above requirement?",
        "relatedLectureIds": "",
        "answers": [
          "Make a snapshot of the database",
          "Enabled Multi-AZ failover ",
          "Increase the database instance size",
          "Create a read replica"
        ]
      },
      "correct_response": ["b"],
      "section": "Design Resilient Architectures",
      "question_plain": "There are a lot of outages in the Availability Zone of your RDS database instance to the point that you have lost access to the database. What could you do to prevent losing access to your database in case that this event happens again?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264476,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "An IT consultant is working for a large financial company. The role of the consultant is to help the development team build a highly available web application using stateless web servers.In this scenario, which AWS services are suitable for storing session state data? (Select TWO.)",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "DynamoDB and ElastiCache are the correct answers. You can store session state data on both DynamoDB and ElastiCache. These AWS services provide high-performance storage of key-value pairs which can be used to build a highly available web application.",
        "relatedLectureIds": "",
        "answers": [
          "Redshift Spectrum",
          "DynamoDB",
          "RDS",
          "ElastiCache",
          "Glacier"
        ]
      },
      "correct_response": ["b", "d"],
      "section": "Design Resilient Architectures",
      "question_plain": "An IT consultant is working for a large financial company. The role of the consultant is to help the development team build a highly available web application using stateless web servers.In this scenario, which AWS services are suitable for storing session state data? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264478,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "A popular social media website uses a CloudFront web distribution to serve their static contents to their millions of users around the globe. They are receiving a number of complaints recently that their users take a lot of time to log into their website. There are also occasions when their users are getting HTTP 504 errors. You are instructed by your manager to significantly reduce the user's login time to further optimize the system. Which of the following options should you use together to set up a cost-effective solution that can improve your application's performance? (Select TWO.)",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:- After CloudFront receives a request from a viewer (viewer request)- Before CloudFront forwards the request to the origin (origin request)- After CloudFront receives the response from the origin (origin response)- Before CloudFront forwards the response to the viewer (viewer response)",
        "relatedLectureIds": "",
        "answers": [
          "Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.",
          "Use multiple and geographically disperse VPCs to various AWS regions then create a transit VPC to connect all of your resources. In order to handle the requests faster, set up Lambda functions in each region using the AWS Serverless Application Model (SAM) service.",
          "Configure your origin to add a  to increase the cache hit ratio of your CloudFront distribution.",
          "Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user.",
          "Set up an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses."
        ]
      },
      "correct_response": ["a", "e"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A popular social media website uses a CloudFront web distribution to serve their static contents to their millions of users around the globe. They are receiving a number of complaints recently that their users take a lot of time to log into their website. There are also occasions when their users are getting HTTP 504 errors. You are instructed by your manager to significantly reduce the user's login time to further optimize the system. Which of the following options should you use together to set up a cost-effective solution that can improve your application's performance? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264480,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company has a hybrid cloud architecture that connects their on-premises data center and cloud infrastructure in AWS. They require a durable storage backup for their corporate documents stored on-premises and a local cache that provides low latency access to their recently accessed data to reduce data egress charges. The documents must be stored to and retrieved from AWS via the Server Message Block (SMB) protocol. These files must immediately be accessible within minutes for six months and archived for another decade to meet the data compliance. Which of the following is the best and most cost-effective approach to implement in this scenario?",
        "feedbacks": ["", "", "", ""],
        "explanation": "A file gateway supports a file interface into Amazon Simple Storage Service (Amazon S3) and combines a service and a virtual software appliance. By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). The software appliance, or gateway, is deployed into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or Linux Kernel-based Virtual Machine (KVM) hypervisor.",
        "relatedLectureIds": "",
        "answers": [
          "Launch a new file gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the file gateway and set up a lifecycle policy to move the data into Glacier for data archival.",
          "Launch a new tape gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the tape gateway and set up a lifecycle policy to move the data into Glacier for archival.",
          "Establish a Direct Connect connection to integrate your on-premises network to your VPC. Upload the documents on Amazon EBS Volumes and use a lifecycle policy to automatically move the EBS snapshots to an S3 bucket, and then later to Glacier for archival.",
          "Use AWS Snowmobile to migrate all of the files from the on-premises network. Upload the documents to an S3 bucket and set up a lifecycle policy to move the data into Glacier for archival."
        ]
      },
      "correct_response": ["a"],
      "section": "Design Resilient Architectures",
      "question_plain": "A company has a hybrid cloud architecture that connects their on-premises data center and cloud infrastructure in AWS. They require a durable storage backup for their corporate documents stored on-premises and a local cache that provides low latency access to their recently accessed data to reduce data egress charges. The documents must be stored to and retrieved from AWS via the Server Message Block (SMB) protocol. These files must immediately be accessible within minutes for six months and archived for another decade to meet the data compliance. Which of the following is the best and most cost-effective approach to implement in this scenario?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264482,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "A company conducted a surprise IT audit on all of the AWS resources being used in the production environment. During the audit activities, it was noted that you are using a combination of Standard and Convertible Reserved EC2 instances in your applications. They argued that you should have used Spot EC2 instances instead as it is cheaper than the Reserved Instance.Which of the following are the characteristics and benefits of using these two types of Reserved EC2 instances, which you can use as justification? (Select TWO.)",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "Reserved Instances (RIs) provide you with a significant discount (up to 75%) compared to On-Demand instance pricing. You have the flexibility to change families, OS types, and tenancies while benefiting from RI pricing when you use Convertible RIs. One important thing to remember here is that Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account.The offering class of a Reserved Instance is either Standard or Convertible. A Standard Reserved Instance provides a more significant discount than a Convertible Reserved Instance, but you can't exchange a Standard Reserved Instance unlike Convertible Reserved Instances. You can modify Standard and Convertible Reserved Instances. Take note that in Convertible Reserved Instances, you are allowed to exchange another Convertible Reserved instance with a different instance type and tenancy.The configuration of a Reserved Instance comprises a single instance type, platform, scope, and tenancy over a term. If your computing needs change, you might be able to modify or exchange your Reserved Instance.When your computing needs change, you can modify your Standard or Convertible Reserved Instances and continue to take advantage of the billing benefit. You can modify the Availability Zone, scope, network platform, or instance size (within the same instance type) of your Reserved Instance. You can also sell your unused instance on the Reserved Instance Marketplace.Hence, the correct options are:- Reserved Instances don't get interrupted unlike Spot instances in the event that there are not enough unused EC2 instances to meet the demand- Convertible Reserved Instances allows you to exchange for another Convertible Reserved instance with a different instance type and tenancy.The option that says: Standard Reserved Instances can be later exchanged for other Convertible Reserved Instances is incorrect because only Convertible Reserved Instances can be exchanged for other Convertible Reserved Instances.The option that says: It can enable you to reserve capacity for your Amazon EC2 instances in multiple Availability Zones and multiple AWS Regions for any duration is incorrect because you can reserve capacity to a specific AWS Region (regional Reserved Instance) or specific Availability Zone (zonal Reserved Instance) only. You cannot reserve capacity to multiple AWS Regions in a single RI purchase.The option that says: It runs in a VPC on hardware that's dedicated to a single customer is incorrect because that is the description of a Dedicated instance and not a Reserved Instance. A Dedicated instance runs in a VPC on hardware that's dedicated to a single customer.References:",
        "relatedLectureIds": "",
        "answers": [
          "Standard Reserved Instances can be later exchanged for other Convertible Reserved Instances",
          "It can enable you to reserve capacity for your Amazon EC2 instances in multiple Availability Zones and multiple AWS Regions for any duration.",
          "Reserved Instances don't get interrupted unlike Spot instances in the event that there are not enough unused EC2 instances to meet the demand.",
          "It runs in a VPC on hardware that's dedicated to a single customer.",
          "Convertible Reserved Instances allow you to exchange for another Convertible Reserved instance with a different instance type and tenancy."
        ]
      },
      "correct_response": ["c", "e"],
      "section": "Design Resilient Architectures",
      "question_plain": "A company conducted a surprise IT audit on all of the AWS resources being used in the production environment. During the audit activities, it was noted that you are using a combination of Standard and Convertible Reserved EC2 instances in your applications. They argued that you should have used Spot EC2 instances instead as it is cheaper than the Reserved Instance.Which of the following are the characteristics and benefits of using these two types of Reserved EC2 instances, which you can use as justification? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264484,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using DynamoDB. You are instructed to improve the database performance by distributing the workload evenly and using the provisioned throughput efficiently.    Which of the following would you consider to implement for your DynamoDB table?",
        "relatedLectureIds": "",
        "explanation": "The partition key portion of a table's primary key determines the logical partitions in which a table's data is stored. This in turn affects the underlying physical partitions. Provisioned I/O capacity for the table is divided evenly among these physical partitions. Therefore a partition key design that doesn't distribute I/O requests evenly can create \"hot\" partitions that result in throttling and use your provisioned I/O capacity inefficiently.The optimal usage of a table's provisioned throughput depends not only on the workload patterns of individual items, but also on the partition-key design. This doesn't mean that you must access all partition key values to achieve an efficient throughput level, or even that the percentage of accessed partition key values must be high. It does mean that the more distinct partition key values that your workload accesses, the more those requests will be spread across the partitioned space. In general, you will use your provisioned throughput more efficiently as the ratio of partition key values accessed to the total number of partition key values increases.One example for this is the use of partition keys with high-cardinality attributes, which have a large number of distinct values for each item.Reducing the number of partition keys in the DynamoDB table is incorrect. Instead of doing this, you should actually add more to improve its performance to distribute the I/O requests evenly and not avoid \"hot\" partitions.Using partition keys with low-cardinality attributes, which have a few number of distinct values for each item is incorrect because this is the exact opposite of the correct answer. Remember that the more distinct partition key values your workload accesses, the more those requests will be spread across the partitioned space. Conversely, the less distinct partition key values, the less evenly spread it would be across the partitioned space, which effectively slows the performance.The option that says: Avoid using a composite primary key, which is composed of a partition key and a sort key is incorrect because as mentioned, a composite primary key will provide more partition for the table and in turn, improves the performance. Hence, it should be used and not avoided.References:",
        "feedbacks": ["", "", "", ""],
        "answers": [
          "Reduce the number of partition keys in the DynamoDB table.",
          "Use partition keys with high-cardinality attributes, which have a large number of distinct values for each item.",
          "Use partition keys with low-cardinality attributes, which have a few number of distinct values for each item.",
          "Avoid using a composite primary key, which is composed of a partition key and a sort key."
        ]
      },
      "correct_response": ["b"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using DynamoDB. You are instructed to improve the database performance by distributing the workload evenly and using the provisioned throughput efficiently.    Which of the following would you consider to implement for your DynamoDB table?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264486,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company is designing a banking portal that uses Amazon ElastiCache for Redis as its distributed session management component. Since the other Cloud Engineers in your department have access to your ElastiCache cluster, you have to secure the session data in the portal by requiring them to enter a password before they are granted permission to execute Redis commands.As the Solutions Architect, which of the following should you do to meet the above requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Using Redis ",
        "relatedLectureIds": "",
        "answers": [
          "Set up an IAM Policy and MFA which requires the Cloud Engineers to enter their IAM credentials and token before they can access the ElastiCache cluster.",
          "Set up a Redis replication group and enable the  parameter.",
          "Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the  parameters enabled.",
          "Enable the in-transit encryption for Redis replication groups."
        ]
      },
      "correct_response": ["c"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A company is designing a banking portal that uses Amazon ElastiCache for Redis as its distributed session management component. Since the other Cloud Engineers in your department have access to your ElastiCache cluster, you have to secure the session data in the portal by requiring them to enter a password before they are granted permission to execute Redis commands.As the Solutions Architect, which of the following should you do to meet the above requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264492,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A Solutions Architect identified a series of DDoS attacks while monitoring the VPC. The Architect needs to fortify the current cloud infrastructure to protect the data of the clients.Which of the following is the most suitable solution to mitigate these kinds of attacks?",
        "feedbacks": ["", "", "", ""],
        "explanation": "For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing(ELB), Amazon CloudFront, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall.",
        "relatedLectureIds": "",
        "answers": [
          "Use AWS Shield Advanced to detect and mitigate DDoS attacks.",
          "Using the AWS Firewall Manager, set up a security layer that will prevent SYN floods, UDP reflection attacks, and other DDoS attacks.",
          "Set up a web application firewall using AWS WAF to filter, monitor, and block HTTP traffic.",
          "A combination of Security Groups and Network Access Control Lists to only allow authorized traffic to access your VPC."
        ]
      },
      "correct_response": ["a"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A Solutions Architect identified a series of DDoS attacks while monitoring the VPC. The Architect needs to fortify the current cloud infrastructure to protect the data of the clients.Which of the following is the most suitable solution to mitigate these kinds of attacks?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264494,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An AI-powered Forex trading application consumes thousands of data sets to train its machine learning model. The application’s workload requires a high-performance, parallel hot storage to process the training datasets concurrently. It also needs cost-effective cold storage to archive those datasets that yield low profit.Which of the following Amazon storage services should the developer use?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Hot storage refers to the storage that keeps frequently accessed data ( hot data ). Warm storage refers to the storage that keeps less frequently accessed data ( warm data ). Cold storage refers to the storage that keeps rarely accessed data ( cold data ). In terms of pricing, the colder the data, the cheaper it is to store, and the costlier it is to access when needed.",
        "relatedLectureIds": "",
        "answers": [
          "Use Amazon FSx For Lustre and Amazon EBS Provisioned IOPS SSD (io1) volumes for hot and cold storage respectively.",
          "Use Amazon FSx For Lustre and Amazon S3 for hot and cold storage respectively.",
          "Use Amazon Elastic File System and Amazon S3 for hot and cold storage respectively.",
          "Use Amazon FSx For Windows File Server and Amazon S3 for hot and cold storage respectively."
        ]
      },
      "correct_response": ["b"],
      "section": "Design High-Performing Architectures",
      "question_plain": "An AI-powered Forex trading application consumes thousands of data sets to train its machine learning model. The application’s workload requires a high-performance, parallel hot storage to process the training datasets concurrently. It also needs cost-effective cold storage to archive those datasets that yield low profit.Which of the following Amazon storage services should the developer use?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264496,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An application consists of multiple EC2 instances in private subnets in different availability zones. The application uses a single NAT Gateway for downloading software patches from the Internet to the instances. There is a requirement to protect the application from a single point of failure when the NAT Gateway encounters a failure or if its availability zone goes down.How should the Solutions Architect redesign the architecture to be more highly available and cost-effective",
        "feedbacks": ["", "", "", ""],
        "explanation": "A NAT Gateway is a highly available, managed Network Address Translation (NAT) service for your resources in a private subnet to access the Internet. NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone.You must create a NAT gateway on a public subnet to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances.",
        "relatedLectureIds": "",
        "answers": [
          "Create a NAT Gateway in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone",
          "Create a NAT Gateway in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.",
          "Create two NAT Gateways in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.",
          "Create three NAT Gateways in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone."
        ]
      },
      "correct_response": ["a"],
      "section": "Design Resilient Architectures",
      "question_plain": "An application consists of multiple EC2 instances in private subnets in different availability zones. The application uses a single NAT Gateway for downloading software patches from the Internet to the instances. There is a requirement to protect the application from a single point of failure when the NAT Gateway encounters a failure or if its availability zone goes down.How should the Solutions Architect redesign the architecture to be more highly available and cost-effective",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264498,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company plans to launch an Amazon EC2 instance in a private subnet for its internal corporate web portal. For security purposes, the EC2 instance must send data to Amazon DynamoDB and Amazon S3 via private endpoints that don't pass through the public Internet.Which of the following can meet the above requirements?",
        "feedbacks": ["", "", "", ""],
        "explanation": "A VPC endpoint allows you to privately connect your VPC to supported AWS and VPC endpoint services powered by AWS PrivateLink without needing an Internet gateway, NAT computer, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.",
        "relatedLectureIds": "",
        "answers": [
          "Use VPC endpoints to route all access to S3 and DynamoDB via private endpoints.",
          "Use AWS VPN CloudHub to route all access to S3 and DynamoDB via private endpoints.",
          "Use AWS Transit Gateway to route all access to S3 and DynamoDB via private endpoints.",
          "Use AWS Direct Connect to route all access to S3 and DynamoDB via private endpoints."
        ]
      },
      "correct_response": ["a"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A company plans to launch an Amazon EC2 instance in a private subnet for its internal corporate web portal. For security purposes, the EC2 instance must send data to Amazon DynamoDB and Amazon S3 via private endpoints that don't pass through the public Internet.Which of the following can meet the above requirements?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264500,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An organization needs a persistent block storage volume that will be used for mission-critical workloads. The backup data will be stored in an object storage service and after 30 days, the data will be stored in a data archiving storage service.What should you do to meet the above requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon Elastic Block Store (EBS) is an easy-to-use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale. A broad range of workloads, such as relational and non-relational databases, enterprise applications, containerized applications, big data analytics engines, file systems, and media workflows are widely deployed on Amazon EBS.Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics.In an S3 Lifecycle configuration, you can define rules to transition objects from one storage class to another to save on storage costs. Amazon S3 supports a waterfall model for transitioning between storage classes, as shown in the diagram below:",
        "relatedLectureIds": "",
        "answers": [
          "Attach an EBS volume in your EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to Amazon S3 Glacier.",
          "Attach an EBS volume in your EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to Amazon S3 One Zone-IA.",
          "Attach an instance store volume in your existing EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to Amazon S3 Glacier.",
          "Attach an instance store volume in your EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to Amazon S3 One Zone-IA."
        ]
      },
      "correct_response": ["a"],
      "section": "Design Resilient Architectures",
      "question_plain": "An organization needs a persistent block storage volume that will be used for mission-critical workloads. The backup data will be stored in an object storage service and after 30 days, the data will be stored in a data archiving storage service.What should you do to meet the above requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264502,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A Solutions Architect designed a serverless architecture that allows AWS Lambda to access an Amazon DynamoDB table named tutorialsdojo in the US East (N. Virginia) region. The IAM policy attached to a Lambda function allows it to put and delete items in the table. The policy must be updated to only allow two operations in the tutorialsdojo table and prevent other DynamoDB tables from being modified.Which of the following IAM policies fulfill this requirement and follows the principle of granting the least privilege? ",
        "feedbacks": ["", "", "", ""],
        "explanation": "Every AWS resource is owned by an AWS account, and permissions to create or access a resource are governed by permissions policies. An account administrator can attach permissions policies to IAM identities (that is, users, groups, and roles), and some services (such as AWS Lambda) also support attaching permissions policies to resources.In DynamoDB, the primary resources are tables. DynamoDB also supports additional resource types, indexes, and streams. However, you can create indexes and streams only in the context of an existing DynamoDB table. These are referred to as subresources. These resources and subresources have unique Amazon Resource Names (ARNs) associated with them.For example, an AWS Account (123456789012) has a DynamoDB table named ",
        "relatedLectureIds": "",
        "answers": ["", "", "", ""]
      },
      "correct_response": ["a"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A Solutions Architect designed a serverless architecture that allows AWS Lambda to access an Amazon DynamoDB table named tutorialsdojo in the US East (N. Virginia) region. The IAM policy attached to a Lambda function allows it to put and delete items in the table. The policy must be updated to only allow two operations in the tutorialsdojo table and prevent other DynamoDB tables from being modified.Which of the following IAM policies fulfill this requirement and follows the principle of granting the least privilege?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264504,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company requires all the data stored in the cloud to be encrypted at rest. To easily integrate this with other AWS services, they must have full control over the encryption of the created keys and also the ability to immediately remove the key material from AWS KMS. The solution should also be able to audit the key usage independently of AWS CloudTrail.Which of the following options will meet this requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "The AWS Key Management Service (KMS) custom key store feature combines the controls provided by AWS CloudHSM with the integration and ease of use of AWS KMS. You can configure your own CloudHSM cluster and authorize AWS KMS to use it as a dedicated key store for your keys rather than the default AWS KMS key store. When you create keys in AWS KMS you can choose to generate the key material in your CloudHSM cluster. CMKs that are generated in your custom key store never leave the HSMs in the CloudHSM cluster in plaintext and all AWS KMS operations that use those keys are only performed in your HSMs.",
        "relatedLectureIds": "",
        "answers": [
          "Use AWS Key Management Service to create AWS-owned CMKs and store the non-extractable key material in AWS CloudHSM.",
          "Use AWS Key Management Service to create a CMK in a custom key store and store the non-extractable key material in Amazon S3.",
          "Use AWS Key Management Service to create AWS-managed CMKs and store the non-extractable key material in AWS CloudHSM.",
          "Use AWS Key Management Service to create a CMK in a custom key store and store the non-extractable key material in AWS CloudHSM."
        ]
      },
      "correct_response": ["d"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A company requires all the data stored in the cloud to be encrypted at rest. To easily integrate this with other AWS services, they must have full control over the encryption of the created keys and also the ability to immediately remove the key material from AWS KMS. The solution should also be able to audit the key usage independently of AWS CloudTrail.Which of the following options will meet this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264506,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "An organization needs to provision a new Amazon EC2 instance with a persistent block storage volume to migrate data from its on-premises network to AWS. The required maximum performance for the storage volume is 64,000 IOPS.In this scenario, which of the following can be used to fulfill this requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible.The AWS Nitro System is the underlying platform for the latest generation of EC2 instances that enables AWS to innovate faster, further reduce the cost of the customers, and deliver added benefits like increased security and new instance types.",
        "relatedLectureIds": "",
        "answers": [
          "Directly attach multiple Instance Store volumes in an EC2 instance to deliver maximum IOPS performance.",
          "Launch a Nitro-based EC2 instance and attach a Provisioned IOPS SSD EBS volume (io1) with 64,000 IOPS.",
          "Launch an Amazon EFS file system and mount it to a Nitro-based Amazon EC2 instance and set the performance mode to Max I/O.",
          "Launch any type of Amazon EC2 instance and attach a Provisioned IOPS SSD EBS volume (io1) with 64,000 IOPS."
        ]
      },
      "correct_response": ["b"],
      "section": "Design High-Performing Architectures",
      "question_plain": "An organization needs to provision a new Amazon EC2 instance with a persistent block storage volume to migrate data from its on-premises network to AWS. The required maximum performance for the storage volume is 64,000 IOPS.In this scenario, which of the following can be used to fulfill this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264508,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company needs to deploy at least 2 EC2 instances to support the normal workloads of its application and automatically scale up to 6 EC2 instances to handle the peak load. The architecture must be highly available and fault-tolerant as it is processing mission-critical workloads.As the Solutions Architect of the company, what should you do to meet the above requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon EC2 Auto Scaling helps ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. You can also specify the maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes above this size.",
        "relatedLectureIds": "",
        "answers": [
          "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Deploy 4 instances in Availability Zone A.",
          "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 4 and the maximum capacity to 6. Deploy 2 instances in Availability Zone A and another 2 instances in Availability Zone B.",
          "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Use 2 Availability Zones and deploy 1 instance for each AZ.",
          "Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 4. Deploy 2 instances in Availability Zone A and 2 instances in Availability Zone B."
        ]
      },
      "correct_response": ["b"],
      "section": "Design Resilient Architectures",
      "question_plain": "A company needs to deploy at least 2 EC2 instances to support the normal workloads of its application and automatically scale up to 6 EC2 instances to handle the peak load. The architecture must be highly available and fault-tolerant as it is processing mission-critical workloads.As the Solutions Architect of the company, what should you do to meet the above requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264510,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company has a web application that uses Internet Information Services (IIS) for Windows Server. A file share is used to store the application data on the network-attached storage of the company’s on-premises data center. To achieve a highly available system, they plan to migrate the application and file share to AWS.Which of the following can be used to fulfill this requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon FSx for Windows File Server provides fully managed Microsoft Windows file servers, backed by a fully native Windows file system. Amazon FSx for Windows File Server has the features, performance, and compatibility to easily lift and shift enterprise applications to the AWS Cloud. It is accessible from Windows, Linux, and macOS compute instances and devices. Thousands of compute instances and devices can access a file system concurrently.",
        "relatedLectureIds": "",
        "answers": [
          "Migrate the existing file share configuration to AWS Storage Gateway.",
          "Migrate the existing file share configuration to Amazon FSx for Windows File Server.",
          "Migrate the existing file share configuration to Amazon EFS.",
          "Migrate the existing file share configuration to Amazon EBS."
        ]
      },
      "correct_response": ["b"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A company has a web application that uses Internet Information Services (IIS) for Windows Server. A file share is used to store the application data on the network-attached storage of the company’s on-premises data center. To achieve a highly available system, they plan to migrate the application and file share to AWS.Which of the following can be used to fulfill this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264512,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A Solutions Architect needs to set up a relational database and come up with a disaster recovery plan to mitigate multi-region failure. The solution requires a Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute.Which of the following AWS services can fulfill this requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.",
        "relatedLectureIds": "",
        "answers": [
          "Amazon DynamoDB global tables",
          "Amazon RDS for PostgreSQL with cross-region read replicas",
          "AWS Global Accelerator",
          "Amazon Aurora Global Database"
        ]
      },
      "correct_response": ["d"],
      "section": "Design Resilient Architectures",
      "question_plain": "A Solutions Architect needs to set up a relational database and come up with a disaster recovery plan to mitigate multi-region failure. The solution requires a Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute.Which of the following AWS services can fulfill this requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264514,
      "assessment_type": "multi-select",
      "prompt": {
        "question": "A company plans to host a web application in an Auto Scaling group of Amazon EC2 instances. The application will be used globally by users to upload and store several types of files. Based on user trends, files that are older than 2 years must be stored in a different storage class. The Solutions Architect of the company needs to create a cost-effective and scalable solution to store the old files yet still provide durability and high availability.Which of the following approach can be used to fulfill this requirement? (Select TWO.)",
        "feedbacks": ["", "", "", "", ""],
        "explanation": "Amazon S3 stores data as objects within buckets. An object is a file and any optional metadata that describes the file. To store a file in Amazon S3, you upload it to a bucket. When you upload a file as an object, you can set permissions on the object and any metadata. Buckets are containers for objects. You can have one or more buckets. You can control access for each bucket, deciding who can create, delete, and list objects in it. You can also choose the geographical region where Amazon S3 will store the bucket and its contents and view access logs for the bucket and its objects.",
        "relatedLectureIds": "",
        "answers": [
          "Use Amazon S3 and create a lifecycle policy that will move the objects to Amazon S3 Glacier after 2 years.",
          "Use Amazon EFS and create a lifecycle policy that will move the objects to Amazon EFS-IA after 2 years.",
          "Use Amazon S3 and create a lifecycle policy that will move the objects to Amazon S3 Standard-IA after 2 years.",
          "Use Amazon EBS volumes to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years.",
          "Use a RAID 0 storage configuration that stripes multiple Amazon EBS volumes together to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years."
        ]
      },
      "correct_response": ["a", "c"],
      "section": "Design Resilient Architectures",
      "question_plain": "A company plans to host a web application in an Auto Scaling group of Amazon EC2 instances. The application will be used globally by users to upload and store several types of files. Based on user trends, files that are older than 2 years must be stored in a different storage class. The Solutions Architect of the company needs to create a cost-effective and scalable solution to store the old files yet still provide durability and high availability.Which of the following approach can be used to fulfill this requirement? (Select TWO.)",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264516,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company collects atmospheric data such as temperature, air pressure, and humidity from different countries. Each site location is equipped with various weather instruments and a high-speed Internet connection. The average collected data in each location is around 500 GB and will be analyzed by a weather forecasting application hosted in Northern Virginia. As the Solutions Architect, you need to aggregate all the data in the fastest way.Which of the following options can satisfy the given requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon S3 is object storage built to store and retrieve any amount of data from anywhere on the Internet. It’s a simple storage service that offers industry-leading durability, availability, performance, security, and virtually unlimited scalability at very low costs. Amazon S3 is also designed to be highly flexible. Store any type and amount of data that you want; read the same piece of data a million times or only for emergency disaster recovery; build a simple FTP application or a sophisticated web application.",
        "relatedLectureIds": "",
        "answers": [
          "Enable Transfer Acceleration in the destination bucket and upload the collected data using Multipart Upload.",
          "Upload the data to the closest S3 bucket. Set up a cross-region replication and copy the objects to the destination bucket.",
          "Use AWS Snowball Edge to transfer large amounts of data.",
          "Set up a Site-to-Site VPN connection."
        ]
      },
      "correct_response": ["a"],
      "section": "Design High-Performing Architectures",
      "question_plain": "A company collects atmospheric data such as temperature, air pressure, and humidity from different countries. Each site location is equipped with various weather instruments and a high-speed Internet connection. The average collected data in each location is around 500 GB and will be analyzed by a weather forecasting application hosted in Northern Virginia. As the Solutions Architect, you need to aggregate all the data in the fastest way.Which of the following options can satisfy the given requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264518,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company plans to migrate its on-premises workload to AWS. The current architecture is composed of a Microsoft SharePoint server that uses a Windows shared file storage. The Solutions Architect needs to use a cloud storage solution that is highly available and can be integrated with Active Directory for access control and authentication.Which of the following options can satisfy the given requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. Amazon FSx is accessible from Windows, Linux, and MacOS compute instances and devices. Thousands of compute instances and devices can access a file system concurrently.",
        "relatedLectureIds": "",
        "answers": [
          "Launch an Amazon EC2 Windows Server to mount a new S3 bucket as a file volume.",
          "Create a file system using Amazon EFS and join it to an Active Directory domain.",
          "Create a Network File System (NFS) file share using AWS Storage Gateway.",
          "Create a file system using Amazon FSx for Windows File Server and join it to an Active Directory domain in AWS."
        ]
      },
      "correct_response": ["d"],
      "section": "Design Resilient Architectures",
      "question_plain": "A company plans to migrate its on-premises workload to AWS. The current architecture is composed of a Microsoft SharePoint server that uses a Windows shared file storage. The Solutions Architect needs to use a cloud storage solution that is highly available and can be integrated with Active Directory for access control and authentication.Which of the following options can satisfy the given requirement?",
      "related_lectures": []
    },
    {
      "_class": "assessment",
      "id": 36264520,
      "assessment_type": "multiple-choice",
      "prompt": {
        "question": "A company hosted an e-commerce website on an Auto Scaling group of EC2 instances behind an Application Load Balancer. The Solutions Architect noticed that the website is receiving a large number of illegitimate external requests from multiple systems with IP addresses that constantly change. To resolve the performance issues, the Solutions Architect must implement a solution that would block the illegitimate requests with minimal impact on legitimate traffic.Which of the following options fulfills this requirement?",
        "feedbacks": ["", "", "", ""],
        "explanation": "AWS WAF is tightly integrated with Amazon CloudFront, the Application Load Balancer (ALB), Amazon API Gateway, and AWS AppSync – services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn’t come at the expense of performance. Blocked requests are stopped before they reach your web servers. When you use AWS WAF on regional services, such as Application Load Balancer, Amazon API Gateway, and AWS AppSync, your rules run in the region and can be used to protect Internet-facing resources as well as internal resources.",
        "relatedLectureIds": "",
        "answers": [
          "Create a regular rule in AWS WAF and associate the web ACL to an Application Load Balancer.",
          "Create a custom network ACL and associate it with the subnet of the Application Load Balancer to block the offending requests.",
          "Create a rate-based rule in AWS WAF and associate the web ACL to an Application Load Balancer.",
          "Create a custom rule in the security group of the Application Load Balancer to block the offending requests."
        ]
      },
      "correct_response": ["c"],
      "section": "Design Secure Applications and Architectures",
      "question_plain": "A company hosted an e-commerce website on an Auto Scaling group of EC2 instances behind an Application Load Balancer. The Solutions Architect noticed that the website is receiving a large number of illegitimate external requests from multiple systems with IP addresses that constantly change. To resolve the performance issues, the Solutions Architect must implement a solution that would block the illegitimate requests with minimal impact on legitimate traffic.Which of the following options fulfills this requirement?",
      "related_lectures": []
    }
  ]
}
